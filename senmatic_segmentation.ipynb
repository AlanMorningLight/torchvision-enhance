{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.external import tifffile\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision_multi import transform_multi\n",
    "from torchvision_multi.datasets import image_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 334) uint8\n",
      "(200, 200) int32\n",
      "\n",
      "\n",
      "(500, 334) uint8\n",
      "(200, 200) int32\n",
      "\n",
      "\n",
      "torch.Size([2, 3, 200, 200])\n",
      "<class 'numpy.ndarray'> (200, 200, 3)\n",
      "<class 'numpy.ndarray'> (200, 200, 3)\n",
      "\n",
      "\n",
      "target <class 'torch.LongTensor'> torch.Size([200, 200])\n",
      "target <class 'torch.LongTensor'> torch.Size([200, 200])\n",
      "\n",
      "\n",
      "(500, 334) uint8\n",
      "(200, 200) int32\n",
      "\n",
      "\n",
      "(500, 334) uint8\n",
      "(200, 200) int32\n",
      "\n",
      "\n",
      "torch.Size([2, 3, 200, 200])\n",
      "<class 'numpy.ndarray'> (200, 200, 3)\n",
      "<class 'numpy.ndarray'> (200, 200, 3)\n",
      "\n",
      "\n",
      "target <class 'torch.LongTensor'> torch.Size([200, 200])\n",
      "target <class 'torch.LongTensor'> torch.Size([200, 200])\n",
      "\n",
      "\n",
      "(500, 334) uint8\n",
      "(200, 200) int32\n",
      "\n",
      "\n",
      "(500, 334) uint8\n",
      "(200, 200) int32\n",
      "\n",
      "\n",
      "torch.Size([2, 3, 200, 200])\n",
      "<class 'numpy.ndarray'> (200, 200, 3)\n",
      "<class 'numpy.ndarray'> (200, 200, 3)\n",
      "\n",
      "\n",
      "target <class 'torch.LongTensor'> torch.Size([200, 200])\n",
      "target <class 'torch.LongTensor'> torch.Size([200, 200])\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADICAIAAAAiOjnJAAAAiklEQVR4nO3BAQEAAACCIP+vbkhA\nAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAADwYNWXAAG9rB+hAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=200x200 at 0x7F39B395B908>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.external import tifffile\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from torchvision_multi import transform_multi\n",
    "from torchvision_multi.datasets import image_loader\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(20, 6))\n",
    "# img_tif = tifffile.imread('./sample-data/7-channel.tif')\n",
    "# img_tif_addnoise = transform_multi.noise(img_tif,16,0,0.001)\n",
    "#\n",
    "# img_jpg = Image.open('./sample-data/2007_000129.jpg')\n",
    "# img_tif_addnoise = transform_multi.noise(img_tif,16)\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "transform = transform_multi.SegCompose([\n",
    "#     transform_multi.SegRandomRotate(1),\n",
    "#     transform_multi.SegRandomFlip(),\n",
    "#     transform_multi.SegRandomCrop((300,300)),\n",
    "#     transform_multi.SegRandomShift(1, 30, 30),\n",
    "#       transform_multi.SegRandomNoise(1,8,0,0.01),\n",
    "#         transform_multi.SegPad([(30,30),(30,30),(0,0)],\"reflect\"),\n",
    "       transform_multi.SegResize((200,200)),\n",
    "#       transform_multi.SegCenterCrop((200,200)),\n",
    "#       transform_multi.SegGaussianBlur(1),\n",
    "#     transform_multi.SegPieceTransfor(1,warp_left_right=10,warp_up_down=10),\n",
    "    transform_multi.SegToTensor()\n",
    "\n",
    "#     transform_multi.Lambda(lambda x: transform_multi.to_tensor(x))\n",
    "])\n",
    "\n",
    "trainset = image_loader.SemanticSegmentationLoader(\n",
    "    rootdir='./sample-data/', lstpath='./sample-data/segmentation.lst',\n",
    "    filetype='jpg', transform=transform,\n",
    ")\n",
    "# trainset = image_loader.SemanticSegmentationLoader(\n",
    "#     rootdir='./sample-data/', lstpath='./sample-data/segmentation.lst',\n",
    "#     filetype='jpg', transform=transform,\n",
    "# )\n",
    "\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for step, (inputs, targets) in enumerate(trainloader):\n",
    "    #     print('batch: {} ........'.format(idx))\n",
    "    #     print(type(images), images.shape)\n",
    "    #     print(type(targets), targets.shape)\n",
    "\n",
    "    # Variable, cuda\n",
    "    # net feed forward\n",
    "    # loss\n",
    "    # back propagation\n",
    "\n",
    "    # plot\n",
    "    print(inputs.shape)\n",
    "    for idx, item in enumerate(inputs):\n",
    "        item = torch.squeeze(item, 0)\n",
    "        img_ndarr = transform_multi.to_ndarray(item)\n",
    "        # subplot = int(''.join(str(x) for x in [1, batch_size, idx + 1]))\n",
    "        print(type(img_ndarr), img_ndarr.shape)\n",
    "        img = Image.fromarray(img_ndarr)\n",
    "        target = Image.fromarray(img_ndarr)\n",
    "        # tifffile.imshow(img_ndarr[:, :, [3, 2, 1]], figure=fig, subplot=subplot)\n",
    "    # plt.savefig('./sample-data/plot/{}.png'.format(step), bbox_inches='tight')\n",
    "    print('\\n')\n",
    "    for idx, item in enumerate(targets):\n",
    "        item = torch.squeeze(item, 0)\n",
    "#         img_ndarr = transform_multi.to_ndarray(item)\n",
    "        # subplot = int(''.join(str(x) for x in [1, batch_size, idx + 1]))\n",
    "        print('target', type(item), item.shape)\n",
    "        a = item.numpy()\n",
    "        img_new = a.astype(np.uint8)\n",
    "        target = Image.fromarray(img_new)\n",
    "#         target = Image.fromarray(item)\n",
    "        # tifffile.imshow(img_ndarr[:, :, [3, 2, 1]], figure=fig, subplot=subplot)\n",
    "    # plt.savefig('./sample-data/plot/{}.png'.format(step), bbox_inches='tight')\n",
    "    print('\\n')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = item.numpy()\n",
    "img_new = a.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADICAAAAACIM/FCAAAKfUlEQVR4nNVdx5LsIAxslrlulf7/\nK1U1Zxfv4EQQIJLXry87trFEo0Aw9hpHSMCASc/W4LzfgtBDcI9kDYxEpIuJA7IExiSrIBPpaDqn\nYHFIXsLkRz5NHaJ09xDgXLVUOzJEQKHPV6EvTK2idcgR4TYxWscC0N5IKnzk04wmV27isQJuZoyo\nwXPj3Tnn8q6FFvO3GYQn8thJgEAZ1wLxLF3rcLQ0AcjGCEAMtyTh8xSDBCQYlCeyjskEHjsNuo6p\nYJGVTAaxR8UNJhSJAHglkyS1EFDMWqv64DG4mAczgKpF1tRlwMxCpt+PSxZZM5gYkpny4OO4TGTN\nsGjvx0Iob8vZIzsfucGKdBmnkWYwoFOT11IlomaynEp5IFQnouyK53ApKXKpeL7PVGIE0NbOGDTP\nYlI9jZNHun/q0q8qYZrRtEAAw3UOYRQWeRJE6MyTLyPSP5yYSGTWfJe6mGiIkEaQc7tfTAD1MJll\nkdn9f7O8OYPG4a49BKF9KqS0SLmBJvNAj3fNcK35PDqYzImRWWHuixSYHHMoEROILJpEUiy5HDSv\n6xBvkMCEs6VfTKSNiZJI2aykE9IMamCiIZK7d8fKdRaKxBd0vdm1EDNxhfw4g0hO9gzQxcRJSw83\n/mJdqwu1XldHpBDrwkx6AeqDh1HXWr6kSskPGQoivGqzwlS8PGvdoMr1OhEe1PAQBrOWECIMYC49\np1loWuVapRF3G0hXbEU/YgA48LNOp7FIMWlRdMzHPWamVRQYc62HHsxpVsyqRLhZwW3AqvaJmB0j\nfPxdsSBRxPysdRmEJgqtyxokEivg4+/z+54URPo2nFL7XUMYsUgpZ9GA3C5Mj5G9N5wMqhf5qQwK\ny1fHSk/FZ9dO+RKlEMneVpC3CB8LABv/ifIBMKIKfzYLwO4Hv2L5HMRIeDJE2L/4wYaLyFegkoWQ\nYll/9yAYMEeDHaPsD0DA71EH+8VG4S0NvQhfpalUbA7MqcyFO+gIZ4N62yJKTZzv8x7wLPZ+m33q\n8+NvcSQAVucgxb47f2UFjAE46hA37c0ZHktCXYSJD0IitF2BL91QBvdWqhmxJgd5iMLJjxjlQWHh\nUgf8mC2gNEThZS9EDSOuF/UNGnMG6aDNHerjexyQuBZpBMk87iXiWFFDnZQQWiwiEgleHOrM9fvS\nEBHuoMx8hHM3AJg4/cspaMSe70Mi/aK5PUIYmntIIYkQLQdxdPOinHWq2eUr02sWRwfcta4lqlay\n5sbyWvhEOL6o3zOle6p1KBim4As4I3bOSiOrS8UkhvfYnvCC/aoNBX9idGvmY5fzIDg+QQB8Inw2\nF91l5EqTcM5Urq/CVcMp61qsLDQ/C9592hUjh57N3tP2lncRHYr1ZJSvj8OcOxf4mM5TqD9Vfjww\n+Ean1XMyGVQtccKvk1ffyyJH88cC0wxsHMA2PgsLbEhPa7EJupvgx4g+H8kVtv08YNE3QKLzR3s/\n4k4eBQP4fmYVXncI0oxWOKzKhRIRhhCgF49fupbDIpDnJIxNohuea4uvq0pBRN+uRYBrmJD8njfB\nGGOyPZ21drN7IV/idkJXdwU8i+zhfj/nZ4nHYZB0adVvJw+/AG128856MqOgJLTCu+MTnmZca8Pc\nnfhjV6ftjpOCSMpf8mTfP8Nm+IR66SrOe9FQscNukHiBuAK680N2QN0mMcXHSLtG9mNG1I+cPFq1\nCt2Ohzlx8lMwNlnrG9A5ENFvTXXJH1NCydJmC8j7XRk0Ckp+603I+rq0s8g0VL1DPJ3rytq/VNFU\nGCHYLQqSzQJ22zpCJNreWrTIL2C7ZlJcqsD9rrTB3qO0K0hRtYgdV+PH+p6I/V4lv/1GGLFwdHCX\n2Il8M88O9+HF3A8/HKyO9im8q8xntcPLLixz4rN31U2PQUMxKUvVgoLHhyplMmdCV/FmiFl5OTCy\nOUS96mbzcV5LaMH17SJit4YlPz7+qnyO75/CSDgbg99aVxnK+lwnJCbJiaBmzaGzNUwiy6OBXZoP\nL2tVbGLckSb4PFEq6iP0AelkCpYkhcMlCsqH6bfMxJuv2NL4T6ySLwQQvUyA960jjq8FlQ0+S2Vr\nw9q7jwdsiYlx+SY5hWzHxEYsyHcNSxXysPxtBZ53U9EDfkJntZrd08Zg27aK4FtO0Yfi5bHo3rxB\nYuV9FjHQLkhzrYDIRIdA9qccc78ZPcqnITayt27cxuFvSk7uVQhT46IYMQAPLdjdoPwl9n7HROLN\nQZ1j7GtSXEfFt0ipMbGIoL598JvnoRhPsn9AWpU116JOHg1ulZqEyFN/Ia2Hf1URI80zRJd1K/l0\nzISEX1xTuiDY8zwA6zVLphAD4rbPimNUicQiqyjxOK4jMnMu3i/dLK+m+9A8VljzjUN1EDEy5iD/\n4Ok3Q7XVJ/irK4qWrBOh5nCvVFaQ9vWcS05ZVUz74tmJCTsZGADoIiTrj/XoXGviFlhr4SoCqUfu\nghiph4HkXP5BOJcIDrJIsxap7sthiu0oGvRqsub8d3U7h7zh3Jh8q6iyv0l8S1hy1H9joNYbAgA2\nGKFculzAKJCIvzUx5ftazZDEpQ1ADRJ5erBPmUwdoIYWTIgMPUXQqS2NFkNQ9luaydkXfbxC4kek\nbZ0/ITLT/U4k6ZfG5K2oYx082SKDua176YmttZM7xDGDCE8AuXrTsewUW2QkaT30BYgQ32OZI7YI\nPV6TGiorzKcPzM1aDZ6lK8pqeVM+prejwbNyLLhRM9/LZ5pgF8SPQmm6Bs2RRYRYZ37u5bfGpOc/\nMFW5lpbHw70h+weaL9XoeLgZPDg+ofaFiEhcF71fTepEuKWoX9vYIt9wFQBvfTnUIGr11LU8Joy/\n5cENZYUYOZl8v38zlFUhmfRL/cjX2zHysEH6t7mJHeJJY96G79lIV2Eq6fedkS6tOpWI/IVBKDzU\nN+SLFh/G8DIi8RQx280mCbVM5KUhIqFA5A9CZEDly1yrH+8iQsXDC1LkFIk8HSIjzpwnsv1Pof42\n10rR/1jhxHvHWTLeZJHGV9BC5IjUNpE+A9IXzVvkcR4tBkmnfPIC3YsCRLs1KWuRFzgWWnxLJPKO\nADmgTMAZi7yHhxYSkQciRHpnj6SC0klxTVMO9nUG6WgkXbh/khdSVtvDACue0okx8iKDsLbgTyR7\n26Z84CeP0yC6pTiGtlmjGHmmJ1TzQOZrDQJCIo90IO7QFEF6NZX1Un98mSM81FvTrgaNfLpX8YEf\nY653RJ7s0Kcr+gHMrJfK6yKurz2XX6HwoCb82Qu7cXvoPsJmq6/LdeII9jmiFW97XjQ0vLlB9+NT\n3UwvNezZb5qzqyEZ/i+J1KzAf/R8ZHYIc5PIuTvoKtHekrAYbU3zyv+HyABaLfx3RKL8y95v25HF\nniRS8qyr7rZWNNP/TI3Q8gahpHL54iXKmRciXtmP9IxiphIxFddOajdxHvecRRZPPmcTmVTd9tB9\nMkaSry/PFP4YEfmRpGjALqs+RWRmhIhJey6RZ6b8YqNMt0i26dUk++bCD7mWTM9bwQmK9hh2MhEj\nf4AkWzmTcKzyyFyd79XSS58lPWF5hT3kIdqC8Ew/ilKunfdJZ1WN5FHjP4WzwFwupTrTAAAAAElF\nTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=200x200 at 0x7F39B42F5A90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(img_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img.save(\"mesh_trans.png\",format==\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[255, 15, 0, 2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "img=img_new\n",
    "value = []\n",
    "for i in range(np.array(img).shape[0]):\n",
    "    for j in range(np.array(img).shape[1]):\n",
    "        pixel = np.array(img)[i,j]\n",
    "        if pixel not in value:\n",
    "            value.append(pixel)\n",
    "            \n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[255,  15,  15, ...,   0,   0,   0],\n",
       "       [255,  15,  15, ...,   0,   0,   0],\n",
       "       [255,  15,  15, ...,   0,   0,   0],\n",
       "       ..., \n",
       "       [  0,   0,   0, ...,   2,   2,   2],\n",
       "       [  0,   0,   0, ...,   2,   2,   2],\n",
       "       [  0,   0,   0, ...,   2,   2,   2]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tiff image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.external import tifffile\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from torchvision_multi import transform_multi\n",
    "from torchvision_multi.datasets import image_loader\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20, 6))\n",
    "# img_tif = tifffile.imread('./sample-data/7-channel.tif')\n",
    "# img_tif_addnoise = transform_multi.noise(img_tif,16,0,0.001)\n",
    "#\n",
    "# img_jpg = Image.open('./sample-data/2007_000129.jpg')\n",
    "# img_tif_addnoise = transform_multi.noise(img_tif,16)\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "transform = transform_multi.SegCompose([\n",
    "#     transform_multi.SegRandomRotate(1),\n",
    "#     transform_multi.SegRandomFlip(),\n",
    "#        transform_multi.SegPad([(30,30),(30,30),(0,0)],\"reflect\"),\n",
    "#     transform_multi.SegRandomCrop((400,10)), \n",
    "#     transform_multi.SegRandomShift(1, 30, 30),\n",
    "#       transform_multi.SegRandomNoise(1,8,0,1),\n",
    "#       transform_multi.SegGaussianBlur(1),\n",
    "       transform_multi.SegResize((300,300)),\n",
    "#     transform_multi.SegPieceTransfor(1,warp_left_right=30,warp_up_down=30),\n",
    "    transform_multi.SegToTensor()\n",
    "\n",
    "#     transform_multi.Lambda(lambda x: transform_multi.to_tensor(x))\n",
    "])\n",
    "\n",
    "trainset = image_loader.SemanticSegmentationLoader(\n",
    "    rootdir='./sample-data/', lstpath='./sample-data/segmentation_tiff.lst',\n",
    "    filetype='tif', transform=transform,\n",
    ")\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for step, (inputs, targets) in enumerate(trainloader):\n",
    "    #     print('batch: {} ........'.format(idx))\n",
    "    #     print(type(images), images.shape)\n",
    "    #     print(type(targets), targets.shape)\n",
    "\n",
    "    # Variable, cuda\n",
    "    # net feed forward\n",
    "    # loss\n",
    "    # back propagation\n",
    "\n",
    "    # plot\n",
    "    print(inputs.shape)\n",
    "    for idx, item in enumerate(inputs):\n",
    "        item = torch.squeeze(item, 0)\n",
    "        img_ndarr = transform_multi.to_ndarray(item)\n",
    "        subplot = int(''.join(str(x) for x in [1, batch_size, idx + 1]))\n",
    "        print(type(img_ndarr), img_ndarr.shape)\n",
    "#         img = Image.fromarray(img_ndarr)\n",
    "#         target = Image.fromarray(img_ndarr)\n",
    "        tifffile.imshow(img_ndarr[:, :, [3, 2, 1]], figure=fig, subplot=subplot)\n",
    "    # plt.savefig('./sample-data/plot/{}.png'.format(step), bbox_inches='tight')\n",
    "    print('\\n')\n",
    "    for idx, item in enumerate(targets):\n",
    "        item = torch.squeeze(item, 0)\n",
    "#         img_ndarr = transform_multi.to_ndarray(item)\n",
    "        # subplot = int(''.join(str(x) for x in [1, batch_size, idx + 1]))\n",
    "        item.numpy().dtype\n",
    "        img = Image.fromarray(item.numpy())\n",
    "        print(type(img), img.size)\n",
    "#         target = Image.fromarray(img)\n",
    "        # tifffile.imshow(img_ndarr[:, :, [3, 2, 1]], figure=fig, subplot=subplot)\n",
    "    # plt.savefig('./sample-data/plot/{}.png'.format(step), bbox_inches='tight')\n",
    "    print('\\n')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
